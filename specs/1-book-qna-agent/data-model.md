# Data Model: Book Q&A Retrieval Agent

## Overview
Data models for the retrieval-enabled agent for book Q&A, defining the structure of questions, retrieved content, answers, and metadata.

## Core Entities

### Question
Represents a natural language query from the user about book content.

```python
class Question(BaseModel):
    id: str = Field(description="Unique identifier for the question")
    content: str = Field(description="The natural language question text")
    timestamp: datetime = Field(description="When the question was asked")
    user_id: Optional[str] = Field(description="Identifier for the user (if applicable)")
```

### QueryEmbedding
Represents the vector embedding of a question for semantic search.

```python
class QueryEmbedding(BaseModel):
    question_id: str = Field(description="Reference to the original question")
    embedding: List[float] = Field(description="The embedding vector")
    model: str = Field(description="The model used to generate the embedding")
    timestamp: datetime = Field(description="When the embedding was generated")
```

### RetrievedChunk
Represents a text chunk retrieved from Qdrant based on semantic similarity.

```python
class RetrievedChunk(BaseModel):
    id: str = Field(description="Unique identifier for the chunk")
    content: str = Field(description="The text content of the chunk")
    score: float = Field(description="Similarity score from vector search")
    metadata: Dict[str, Any] = Field(description="Additional metadata (source, page, etc.)")
    source_url: Optional[str] = Field(description="URL or reference to the source")
    page_number: Optional[int] = Field(description="Page number if applicable")
    section_title: Optional[str] = Field(description="Section title if applicable")
    embedding: Optional[List[float]] = Field(description="The embedding vector of the chunk")
```

### Answer
Represents the synthesized response generated by the agent based on retrieved content.

```python
class Answer(BaseModel):
    id: str = Field(description="Unique identifier for the answer")
    question_id: str = Field(description="Reference to the original question")
    content: str = Field(description="The answer text synthesized from retrieved content")
    source_chunks: List[RetrievedChunk] = Field(description="List of chunks used to generate the answer")
    confidence: float = Field(description="Confidence score of the answer")
    timestamp: datetime = Field(description="When the answer was generated")
    generated_by: str = Field(description="The model used to generate the answer")
```

### SourceMetadata
Represents information about where in the book content the answer information originated.

```python
class SourceMetadata(BaseModel):
    source_url: Optional[str] = Field(description="URL reference to the source")
    page_number: Optional[int] = Field(description="Page number in the book")
    section_title: Optional[str] = Field(description="Title of the section")
    chapter_title: Optional[str] = Field(description="Title of the chapter")
    book_title: Optional[str] = Field(description="Title of the book")
    relevance_score: float = Field(description="Relevance score of this source to the question")
```

## Relationships

- `Question` 1 → * `QueryEmbedding` (One question generates one embedding)
- `QueryEmbedding` 1 → * `RetrievedChunk` (One embedding query retrieves multiple chunks)
- `RetrievedChunk` * → 1 `SourceMetadata` (Each chunk has source metadata)
- `Question` 1 → 1 `Answer` (One question gets one answer)
- `Answer` 1 → * `RetrievedChunk` (One answer uses multiple chunks)

## Validation Rules

1. **Question Validation**:
   - Content must not be empty
   - Content must be between 5 and 1000 characters
   - User_id is optional but if provided must be a valid identifier

2. **QueryEmbedding Validation**:
   - Embedding vector must have the correct dimensions for the model
   - Question_id must reference an existing question

3. **RetrievedChunk Validation**:
   - Content must not be empty
   - Score must be between 0 and 1
   - Metadata must contain required fields (source information)

4. **Answer Validation**:
   - Content must not be empty
   - Must reference at least one retrieved chunk
   - Confidence score must be between 0 and 1
   - Content must be derived only from referenced chunks

5. **SourceMetadata Validation**:
   - At least one source identifier (URL, page number, or section title) must be provided
   - Relevance score must be between 0 and 1

## State Transitions

1. **Question Processing Flow**:
   - Question created → Query embedding generated → Chunks retrieved → Answer synthesized → Response returned

2. **Confidence Indicators**:
   - High confidence: Multiple high-scoring chunks with relevant content
   - Low confidence: No relevant chunks found or low similarity scores
   - No answer possible: No chunks meet relevance threshold